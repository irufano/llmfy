# The Next Candidate Features 

## Include rate imit error on exception

## Google LLM (Gemini) boilerplate abstraction
https://ai.google.dev/gemini-api/docs

## Monitor your token usage by counting tokens before running inference:
bedrock:
https://docs.aws.amazon.com/bedrock/latest/userguide/count-tokens.html
openai:
https://github.com/openai/tiktoken
